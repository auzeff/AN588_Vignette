---
title: "Constant-rate birth-death modeling in diversitree"
author: 
  - Sylvie Adams
  - Carly McDermott
  - Christian Mei
  - Akiva Zeff
format:
  html:
    toc : true
    toc-depth: 4
    toc-location: left
    toc-title: "Sections"
editor: visual
theme: journal
---

## What exactly is birth-death modeling?

Today, we'll be exploring constant-rate birth-death modeling in...

------------------------------------------------------------------------

## diversitree

diversitree is a... \[insert eloquent, profound description here\]

```{r}
# Loading diversitree
library(diversitree)
```

Now that we have some background on our models and package, it's time to start by...

------------------------------------------------------------------------

## Making a model

The two parameters for a simple constant-rate birth-death model are $λ$ and $µ$. λ represents the speciation rate in our simulated phylogeny, while µ represents the extinction rate. We'll start by making a tree using these parameters, as well as the maximum number of species we'd like our model to diversity to.

```{r}
# Defining our parameters
lambda <- 0.2 
mu <- 0.05

# Making our tree
set.seed(1)
phylo <- tree.bd(c(lambda, mu), max.taxa = 100) # our tree should terminate at 100 species
```

We now have a tree modeled, but we don't see anything! Let's look a bit further into what exactly our model consists of:

```{r}
str(phylo)
summary(phylo)
```

As we can see, our model takes the form of a list. `summary()` gives us a bit more relevant information about our tree. The labels won't be too important to us right now, but the branch length metrics can tell us something about how quickly our speciation is occurring (we'll investigate this relationship a bit further in Challenge 1).

In order to actually see our tree, we need to try...

------------------------------------------------------------------------

## Plotting our model

```{r}
plot(phylo)
```

We can finally see our tree, but it's a bit busy. We can make it a bit simpler using some optional commands in the plot function:

```{r}
plot(phylo, no.margin = TRUE, show.tip.label = FALSE)
# no.margin allows our plot to take up more of the output window, while show.tip.label gets rid of the labels for our simulated 100 final species
```

------------------------------------------------------------------------

#### Challenge 1

Make a function that takes lambda, mu, and max taxa as inputs and outputs a plot of a simulated phylogeny tree. Lambda and mu should have no default values, while max taxa should default to 100.

::: {.callout-tip collapse="true"}
## Solution

```{r}
phy_plot <- function(lambda, mu, max.taxa = 100) {
  phy <- tree.bd(c(lambda, mu), max.taxa = max.taxa)
  plot(phy, no.margin = TRUE, show.tip.label = FALSE)
}
```
:::

Using your function, experiment with different scenarios.

-   What changes in our model if speciation rate and extinction rate increase/decrease?

-   In what situations is your function returning an error? Why might that be?

    -   Note that as we are simulating a new model each time we run the function, the same parameters will not always have the same result. If you're getting an error, try running it a few times again before giving up!

-   Modify your function to include a summary of the model created. How does changing the parameters affect the mean, variance, and distribution of branch length?

::: {.callout-tip collapse="true"}
## Solution (updated)

```{r}
phy_plot <- function(lambda, mu, max.taxa = 100) {
  phy <- tree.bd(c(lambda, mu), max.taxa = max.taxa)
  plot(phy, no.margin = TRUE, show.tip.label = FALSE)
  summary(phy)
}
```
:::

------------------------------------------------------------------------

Making a cool tree is all well and good, but if we actually want some substantive statistics it's time to start...

------------------------------------------------------------------------

## Analyzing our model

### Likelihood functions

First things first, we need to construct a likelihood function. The likelihood function of a birth-death model estimates the probability of observing a given phylogenetic tree under the assumed rates of speciation and extinction. It will form the backbone of both of our primary analysis methods.

For each branching event occurring at time $t_i$, we define:

-   $p_o(t_i)$: Probability that a lineage goes extinct before time $t_i$

-   $p_o(t_i)$: Probability that a lineage survives and splits by $t_i$

These functions are derived from and allow for model-fitting to empirical phylogenetic data. In our case, these variables will factor into the beautiful and not at all confusing equation derived in Nee et al. (1994):

$$
lik=(N-1)!\lambda^{N-2}\left\{ \prod_{i=3}^{N}P(t_i,T) \right\}(1-u_{x_2})^2\prod_{i=3}^{N}(1-u_{x_i})
$$\
In addition to $t_i$, we see several new variables. $T$ represents the present, $N$ is the total number of lineages in the phylogeny, and $(1-u_{x_i})$ is the probability of a single lineage after an amount of time $x_i$.

This equation is powerful, but in our case it would likely be more useful to use a different derivation of the likelihood, also found in Nee et al. (1994):

$$
lik=(N-1)!(\lambda-\mu)^{N-2}exp\left((\lambda-\mu)\sum_{n=2}^{N-1}x_{n+1}\right)(1-\frac{\mu}{\lambda})^N\prod_{n=2}^{N}\frac{1}{(exp((\lambda-\mu)x_n)-\frac{\mu}{\lambda})^2}
$$

Don't worry, we're not going to go through the whole thing! This equation is longer, but it also contains more familiar components that may be easier to understand conceptually.

$λ$ and $µ$ are recognizable–we used them earlier, and, as we'll see, they're the two parameters taken by the diversitree likelihood function. We also saw $N$ in the earlier equation–remember, it's the total number of lineages in the phylogeny. Apart from those, there is only one other variable that we haven't seen before: $x_n$. It represents the length of time between the present and the birth of the $n$th lineage. We'll delve a bit further into how these variables come together, but first let's actually make our likelihood function. The function to create it in diversitree is `make.bd()` .

```{r}
lik <- make.bd(phylo) # notice that the function is based on our model created above. That model had a seed set for it, so we can expect consistent values for "lik" as well.
```

Our new function takes values of $λ$ and $µ$ as inputs and returns the log likelihood of the parameters. Let's try it with the values we used for our initial tree:

```{r}
lik(c(0.1, 0.03))
```

------------------------------------------------------------------------

#### Challenge 2

We're going to take a break from coding for a moment, and check in on our conceptual understanding. Given the likelihood equation we saw above, why does the function take $λ$ and $µ$ as inputs, but not $N$ and $x_n$?

::: {.callout-tip collapse="true"}
## Solution

Remember that $N$ and $x_n$ are both defined in our tree that we're basing the likelihood function on. Likelihood functions are based on actual "observed" data–in this case our tree–and tell use the probability of seeing that result **under different parameter values**. In the case of a constant-rate birth-death model, $λ$ and $µ$ are the only two parameters. $N$ and $x_n$ are outputted aspects of the simulated phylogeny, and thus can't be changed in the likelihood function.
:::

------------------------------------------------------------------------

One more note: when typing the function out, you may have noticed that it takes another argument, called *condition.surv*. This argument, which is on by default, dictates whether the likelihood is conditional on two lineages surviving to the present. As we can see, turning it off changes the likelihood (we won't get into the math behind that here).

```{r}
lik(c(0.1, 0.03), condition.surv = FALSE)
```

------------------------------------------------------------------------

### Maximum likelihood models

------------------------------------------------------------------------

### Markov chain Monte Carlo

While MLE can help us estimate singular values for λ and µ, it does not mean that it is accurate. MLE gives a single "best-fit" estimate but it does not account for uncertainty. This is especially important when assessing phylogenetic trees, since they can vary in size and can be incomplete. An alternative way is to find a range of values that λ and µ have a high probability of being. This probability distribution is termed a **posterior distribution** and we can arrive to this result by performing Bayesian analysis using the **Markov chain Monte Carlo** **(MCMC)** algorithm.

#### Terminology

Let's break down those terrifying sounding terms and get familiar with them!

The reason we want to employ **Bayesian** analysis on this is because this algorithm will use previous assumptions and the data available to us (likelihood function derived from phylogenetic tree) to arrive at our desired **posterior distribution**.

Previous assumptions are characterized as a **prior distribution** representing existing knowledge or assumptions about model parameters before observing the data. These distributions are probability distributions (e.g., uniform, exponential, normal) that mark the possible ranges for parameters which in our case are speciation (λ) and extinction (μ) rates.

These prior probability distributions affect our posterior distribution. For example, if we had an exponential prior distribution on our parameters, then even if our new data offers much higher values, the model will emphasize our parameters to lie on the lower end but can still permit higher values if the data supports it more. If we don't have very informative prior information, we can assume the prior distribution to be uniform, letting our new data have more say in parameter inference.

This can be seen in **Bayes' Theorem**:

![](images/clipboard-566625555.png)

Source: [freeCodeCamp](https://www.freecodecamp.org/news/bayes-rule-explained/)

What about MCMC? For this, we can break it down into it's two terms: **Markov chain**, and **Monte Carlo**.

A **Markov chain** is a stochastic model (meaning it accounts for randomness) that experiences transitions from one state to the next based on the probabilities of the current state. The easiest way to visualize this is through an image!

![](images/clipboard-3179636823.png)

Source: [geeksforgeeks](https://www.geeksforgeeks.org/markov-chains-in-nlp/)

What we have above is a simple weather model where we have 3 states: *rainy, cloudy, and sunny*. The way to read this is by paying attention to the direction of the arrows. For example, the arrow that self-points to the *sunny* state means that if today is *sunny*, the chances of being *sunny* again is of 0.8. The arrow that points from *sunny* to *cloudy* says that if today is *sunny*, then the probability of it being *cloudy* is of 0.1. Easy!

Using this model, we can perform the chain part of Markov chain. This means that we can create a path or chain of events by "walking" from state to state at random. But remember! In a Markov chain, the probability of transitioning from a current state to a future state solely depends on the current state and not its history of states! Let's do a challenge to see if you understood the concept.

#### Challenge X

Taking the weather model, we tracked the weather states of 7 days beginning on a *rainy* day:

`rainy -> cloudy -> rainy -> sunny -> sunny -> rainy -> rainy -> ...`

What is the probability of it being a *cloudy* day on the 8th day? 0.3

Let's talk about the **Monte Carlo** part of the MCMC. In essence, a **Monte Carlo Simulation** is a model that is used to predict the probability of a range of outcomes when accounting a complex situation where many random variables could influence real life outcomes. It does this by repeated random sampling. Sounds familiar right? That's cause it is very similar in concept to bootstrapping! What differs is that **Monte Carlo** simulates data from a model, while bootstrapping simply re-samples from a dataset.

*Fun fact! the model is named after the famous Monte Carlo Casino in Monaco as a reference to the randomness associated with gambling! Don't gamble, learn stats instead!*

We can use this simple method to solve complex questions!

#### Putting it all together

Now it’s time to merge all of these concepts to define MCMC. This algorithm is a Bayesian approach, meaning that previous assumptions are important to how the new data will be interpreted. The algorithm works by repeatedly proposing random parameter values and evaluating how well they fit the data (using likelihoods and priors, if provided). It then decides whether to accept or reject them using the **Metropolis-Hastings algorithm.** This decision process works by proposing a new sample based on the current sample and then accepting or rejecting it depending on the ratio of the likelihood of the proposed value to the likelihood of the current value. As we repeat this process, due to the acceptance rule, we will slowly accumulate samples that approximate the posterior distribution. This posterior distribution in our case means the range of values our speciation rate (λ) and extinction rate(μ) parameters take.

Enough textbook info and let's see step by step how the algorithm works!

Let's set everything up:

```{r}
set.seed(2)
phy <- tree.bd(c(.1,.03), max.taxa = 100) # simulate phylogenetic tree
lik <- make.bd(phy) # create likelihood function based on simulated tree
```

We can make our initial guesses of λ and μ based on the results from MLE!

```{r}
fit <- find.mle(lik, c(0.1, 0.03), method = "subplex")
coef(fit) # use coef() to see the estimated parameters
```

We can then get the likelihood value of this current value:

```{r}
current_par <- c(coef(fit)[[1]], coef(fit)[[2]])
current_lik <- lik(current_par)
current_lik
```

Our next proposed value can be a random value that is 0.1 steps away. We can denote this step variable as `w` . Please note that parameters in our case cannot be negative since you can't have a negative rate of speciation and extinction.

```{r}
set.seed(2)
w = 0.1
proposal_par <- current_par + runif(2, -w, w)
proposal_par # Make sure that the parameters are not negative!
```

From these proposal parameters we can derive the new likelihood by inserting it to our likelihood function.

```{r}
proposal_lik <- lik(proposal_par)
proposal_lik
```

As directed by the Metropolis-Hastings part of the algorithm, we now need to decide if to accept or reject this new set of parameters. As mentioned we can follow this formula:

$$
r=L(current)/L(proposal)​
$$

But since `lik` gives us the log of likelihood values, we can rework the formula to this:

$$
log r=logL(proposal)−logL(current)
$$

```{r}
log_r <- proposal_lik - current_lik
r <- exp(log_r)
r
```

The `r` value we received is extremely low, meaning that the propsed parameters performed worse than the current parameters. One could automatically discard them, but sometimes these worse values have a place in the final distribution of values. To decide on this, we can write this little conditional:

```{r}
set.seed(1)
if (runif(1) < r) { # we take a random value from a uniform probability distribution that goes from 0 to 1 and compare it to r
      current_par <- proposal_par
      current_lik <- proposal_lik
    }
```

What this code does is:

1.  It takes a random value from 0-1 based on a uniform distribution, meaning that all values are equally probable of being chosen.
2.  If r \> 1, then it will always be accepted and the current parameters and likelihood values will be replaced by the proposed ones.
3.  If r \< 1, then we need to see if it is bigger than the value from `runif(1)`. This means that the proposed values will be accepted with a probability of r

We can see that in this case, the proposed value was rejected:

```{r}
current_lik
```

We repeat this process many many times and will eventually get the posterior distribution!

To put it all together, we come up with our custom MCMC R function:

```{r}
custom_mcmc <- function(likelihood_fn, start, nsteps = 100000, w = c(0.1, 0.1), prior_fn = NULL) {
  # Initialize matrix to hold samples for lambda, mu, and log-likelihood
  samples <- matrix(NA, nrow = nsteps, ncol = 3)
  colnames(samples) <- c("lambda", "mu", "log_lik")
  samples[1, 1:2] <- start # Go to first row and extract values from lambda and mu column
  
  # Current state: lambda and mu
  current_par <- start
  current_lik <- likelihood_fn(current_par)
  
  # Include prior if specified
  if (!is.null(prior_fn)) {
    current_lik <- current_lik + log(prior_fn(current_par))
  }
  
  # Store initial log-likelihood
  samples[1, 3] <- current_lik
  
  for (i in 2:nsteps) {
    # Propose new values for lambda and mu
    proposal_par <- current_par + runif(2, -w, w)
    
    # Check that proposed values are positive and finite (otherwise they won't work in the likelihood function)
    if (any(proposal_par <= 0) || any(!is.finite(proposal_par))) {
      samples[i, ] <- c(current_par, current_lik)  # Reject and stay at current state
      next
    }
    
    # Evaluate likelihood at the proposed values
    proposal_lik <- likelihood_fn(proposal_par)
    
    # Include prior if specified
    if (!is.null(prior_fn)) {
      proposal_lik <- proposal_lik + log(prior_fn(proposal_par))
    }
    
    # Compute acceptance ratio (on log scale)
    log_r <- proposal_lik - current_lik
    r <- exp(log_r)
    
    # Accept or reject the proposal
    if (runif(1) < r) {
      current_par <- proposal_par
      current_lik <- proposal_lik
    }
    
    # Store the current state (either new or previous) and log-posterior
    samples[i, ] <- c(current_par, current_lik)
  }
  
  # Return results as a data frame
  return(as.data.frame(samples))
}
```

Let's use our data to see the posterior distribution we get!

```{r}
start_par <- c(coef(fit)[[1]], coef(fit)[[2]])
test_mcmc <- custom_mcmc(lik, start_par, w = c(0.1, 0.1))
head(test_mcmc)
```

What we get is a table of our posterior distribution with our accepted λ and μ parameters and their likelihood values. We can plot this using:

```{r}
# create r column based on lambda and mu
test_mcmc$r <- test_mcmc$lambda - test_mcmc$mu 

col <- c("#eaab00", "#004165", "#618e02")
profiles.plot(test_mcmc[c("lambda", "mu", "r")], col.line = col, las = 1, legend.pos = "topright")

# Let's see to how the parameters used to simulate the tree compare to our posterior distribution
abline(v=c(0.1, 0.03, 0.07), col = col, lty = 2) 
```

We can see that everything aligns pretty well!

However, in reality, we can skip all of this and use the `mcmc()` function in R to avoid writing our own MCMC algorithm from scratch:

```{r}
set.seed(2)
base_mcmc <- mcmc(lik, start_par, nsteps = 10000, w = 0.1, print.every = 0)
head(base_mcmc)
```

And plot!

```{r}
base_mcmc$r <- base_mcmc$lambda - base_mcmc$mu
col <- c("#eaab00", "#004165", "#618e02")
profiles.plot(base_mcmc[c("lambda", "mu", "r")], col.line = col, las = 1, legend.pos = "topright")
abline(v=c(0.1, 0.03, 0.07), col = col, lty = 2)
```

We can see that both plots are basically identical!
